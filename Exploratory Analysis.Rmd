---
title: "Exploratory Analysis"
author: "Steeg Pierce"
date: "7/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(tidytext)
```

## Broad Strokes

Data included in the corpora from the class's file include text from Twitter, news articles, and blog entries. We can imagine the varied types of writing coming from these sources; the language used in each format will likely be quite different. Given that, we will be able to predict next words more reliably on more kinds of phrasing.

## Goals

Upon completion, the app will be able to take writing as input and return a prediction of the likeliest next word the author of that text would use. To keep memory usage somewhat trim, we will use a Marcov chain style algorithm with n-gram predictions. In other words, for each prediction, we will only be looking at the 'present' set of n words preceding the prediction rather than the entire set. For training, we will be pulling from the aforementioned datasets. With that in mind, it's worth examining the characteristics of said data.

### Reading the Data

```{r readData, message = FALSE}
download.file('https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip', 'SwiftKey.zip')

unzip('SwiftKey.zip')

tweets <- readLines('final/en_US/en_US.twitter.txt')
blogs <- readLines('final/en_US/en_US.blogs.txt')
news <- readLines('final/en_US/en_US.news.txt')

# Find the number of lines in each file

sizes <- c(tweets = length(tweets), blogs = length(blogs), news = length(news))
sizes
```

Here, we download the data and take a quick look at the files. We are restricting ourselves to the English text data as our app will be predicting in English.

### Cleaning the Data

This code tokenizes the text into words so we can more easily examine it.

```{r tokenizeData}
tokenized_tweets <- tibble(text = tweets) %>% 
  unnest_tokens(word, text)
tokenized_blogs <- tibble(text = blogs) %>%
  unnest_tokens(word, text)
tokenized_news <- tibble(text = news) %>%
  unnest_tokens(word, text)
```

Next, we'll take counts of the words.

```{r countWords}
tweet_counts <- tokenized_tweets %>%
  count(word, sort = TRUE)
blog_counts <- tokenized_blogs %>%
  count(word, sort = TRUE)
news_counts <- tokenized_news %>%
  count(word, sort = TRUE)
sum_counts <- bind_rows(tweet_counts, blog_counts, news_counts) %>%
    group_by(word) %>%
    summarize_all(sum) %>%
    arrange(desc(n))
```

### Graphing the Data

We can now use those counts to take a look at word frequencies. Histograms are probably the easiest way to graph the data.

```{r countHistograms, echo = FALSE}
par(mfrow = c(2, 2))
hist(log(tweet_counts$n), xlab = 'Words in Tweets', main = 'Histogram of Tweets')
hist(log(blog_counts$n), xlab = 'Words in Blogs', main = 'Histogram of Blogs')
hist(log(news_counts$n), xlab = 'Words in News', main = 'Histogram of News')
hist(log(sum_counts$n), xlab = 'Words in Combined', main = 'Histogram of All')
```

We can immediately see that a few words make up the vast majority of the dataset. This is true for all 3 corpora and it appears that the same words appear in all of them with some frequency given the similar shape of the combined dataframe.
